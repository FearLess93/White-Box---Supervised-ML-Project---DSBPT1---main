{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09bc1136",
   "metadata": {},
   "source": [
    "\n",
    "# Credit Score Classification — Detailed Report\n",
    "\n",
    "This project uses **Logistic Regression** and **K-Nearest Neighbors (KNN)** to predict a customer’s **credit score category** based on their demographic and financial data.  \n",
    "Each section includes a brief explanation followed by executable code to maintain both clarity and reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac3842",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Statement\n",
    "\n",
    "The objective is to build a machine learning model that classifies customers into categories like *Good*, *Standard*, or *Poor* credit scores.  \n",
    "This helps financial institutions automate the credit scoring process, making it consistent, unbiased, and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62831a82",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Data Loading\n",
    "\n",
    "In this step, we load the provided datasets (`train.csv` and `test.csv`) into pandas DataFrames.  \n",
    "We then preview the data to ensure it’s read correctly and check the column names and basic structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4171a0",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Data Overview\n",
    "\n",
    "Here, we inspect the dataset’s structure, including column data types and number of non-null entries.  \n",
    "This helps us identify which features are numerical or categorical and where missing data may exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa228fa",
   "metadata": {},
   "source": [
    "\n",
    "We also look at descriptive statistics for numerical and categorical columns to understand distributions, ranges, and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec65cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c835c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Target Variable\n",
    "\n",
    "The target variable in this dataset is **Credit_Score**.  \n",
    "Before modeling, it’s important to understand how balanced the classes are, since imbalance can affect model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e50e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['Credit_Score'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67996f85",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Missing Values\n",
    "\n",
    "We now check for missing values in each column.  \n",
    "Identifying and handling missing data early prevents training errors and improves model reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.isna().sum().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754a38a",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Exploratory Data Analysis (EDA)\n",
    "\n",
    "To better understand the dataset, we visualize how many customers fall into each credit score category.  \n",
    "This helps us check for imbalance between the classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e72140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "train['Credit_Score'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Credit Score Distribution\")\n",
    "plt.xlabel(\"Credit Score Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559acda",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Data Preprocessing\n",
    "\n",
    "We prepare the data for modeling by:\n",
    "- Filling missing numeric values with the median.\n",
    "- Filling missing categorical values with the mode.\n",
    "- Converting categorical variables into numerical ones using one-hot encoding.\n",
    "- Splitting the data into training and testing sets for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68186323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['Credit_Score'])\n",
    "y = train['Credit_Score']\n",
    "\n",
    "num_cols = X.select_dtypes(include='number').columns\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
    "X[cat_cols] = X[cat_cols].fillna(X[cat_cols].mode().iloc[0])\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae2c00",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Logistic Regression (Numeric Features Only)\n",
    "\n",
    "We start with a simple Logistic Regression model using **only numeric features**.  \n",
    "This helps us evaluate how well numeric information alone can predict credit scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X_num_train = X_train[num_cols]\n",
    "X_num_test = X_test[num_cols]\n",
    "\n",
    "model_lr1 = LogisticRegression(max_iter=1000)\n",
    "model_lr1.fit(X_num_train, y_train)\n",
    "y_pred_lr1 = model_lr1.predict(X_num_test)\n",
    "\n",
    "acc1 = accuracy_score(y_test, y_pred_lr1)\n",
    "prec1 = precision_score(y_test, y_pred_lr1, average='macro')\n",
    "rec1 = recall_score(y_test, y_pred_lr1, average='macro')\n",
    "f11 = f1_score(y_test, y_pred_lr1, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc1:.4f}, Precision: {prec1:.4f}, Recall: {rec1:.4f}, F1: {f11:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598c5df",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Logistic Regression (All Features)\n",
    "\n",
    "Next, we use **all available features** (numeric + encoded categorical).  \n",
    "This allows the model to leverage more information and potentially improve accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_lr2 = LogisticRegression(max_iter=1000)\n",
    "model_lr2.fit(X_train, y_train)\n",
    "y_pred_lr2 = model_lr2.predict(X_test)\n",
    "\n",
    "acc2 = accuracy_score(y_test, y_pred_lr2)\n",
    "prec2 = precision_score(y_test, y_pred_lr2, average='macro')\n",
    "rec2 = recall_score(y_test, y_pred_lr2, average='macro')\n",
    "f12 = f1_score(y_test, y_pred_lr2, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc2:.4f}, Precision: {prec2:.4f}, Recall: {rec2:.4f}, F1: {f12:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15e7ab",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9: K-Nearest Neighbors (k=3)\n",
    "\n",
    "We now test a KNN model with **k=3**.  \n",
    "This algorithm classifies data based on the majority label among the 3 nearest neighbors in feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5928590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn3.fit(X_scaled_train, y_train)\n",
    "y_pred_k3 = knn3.predict(X_scaled_test)\n",
    "\n",
    "acc3 = accuracy_score(y_test, y_pred_k3)\n",
    "prec3 = precision_score(y_test, y_pred_k3, average='macro')\n",
    "rec3 = recall_score(y_test, y_pred_k3, average='macro')\n",
    "f13 = f1_score(y_test, y_pred_k3, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc3:.4f}, Precision: {prec3:.4f}, Recall: {rec3:.4f}, F1: {f13:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2345c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10: K-Nearest Neighbors (k=5)\n",
    "\n",
    "Here, we increase **k to 5** to check how the model behaves with a larger neighborhood size.  \n",
    "Larger `k` values generally smooth out noise but can reduce sensitivity to small patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X_scaled_train, y_train)\n",
    "y_pred_k5 = knn5.predict(X_scaled_test)\n",
    "\n",
    "acc5 = accuracy_score(y_test, y_pred_k5)\n",
    "prec5 = precision_score(y_test, y_pred_k5, average='macro')\n",
    "rec5 = recall_score(y_test, y_pred_k5, average='macro')\n",
    "f15 = f1_score(y_test, y_pred_k5, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc5:.4f}, Precision: {prec5:.4f}, Recall: {rec5:.4f}, F1: {f15:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6827b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 11: Model Comparison\n",
    "\n",
    "Finally, we compare the performance of all models using key metrics (Accuracy, Precision, Recall, F1).  \n",
    "This helps identify which approach performs best for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacae863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['LR (Numeric)', 'LR (All)', 'KNN (k=3)', 'KNN (k=5)'],\n",
    "    'Accuracy': [acc1, acc2, acc3, acc5],\n",
    "    'Precision': [prec1, prec2, prec3, prec5],\n",
    "    'Recall': [rec1, rec2, rec3, rec5],\n",
    "    'F1': [f11, f12, f13, f15]\n",
    "})\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c85012",
   "metadata": {},
   "source": [
    "\n",
    "## Step 12: Conclusion\n",
    "\n",
    "- **Logistic Regression (All Features)** provides the most balanced and interpretable results.  \n",
    "- **KNN** performs well but is more sensitive to feature scaling and dataset size.  \n",
    "- The best model can be used for automated credit risk classification to improve consistency in decision-making.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Add cross-validation for more stable evaluation.  \n",
    "2. Try adjusting KNN’s `k` value and distance metrics.  \n",
    "3. Consider using Decision Trees or Random Forests for more complex patterns.\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}